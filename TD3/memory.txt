DDPG之前静态场景学习率： critic_lr = 5e-4  # Critic的学习率
        actor_lr = 1e-7   # Actor的学习率
reward+0.05+40.0+0.0+3.0+740.0+0.0+0.0+600.0+-0.0003是按这个训练出来的
reward+0.05+40.0+0.0+2.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 5e-4 actor_lr = 1e-6训练出来的
reward+0.05+40.0+0.0+1.0+740.0+0.0+0.0+600.0+-0.0003是 critic_lr = 5e-4 actor_lr=5e-4练出来的    失败
reward+0.05+40.0+0.0+0.0+740.0+0.0+0.0+600.0+-0.0003是 critic_lr = 5e-4 actor_5e-5练出来的 

reward+0.05+40.0+0.0+6.0+740.0+0.0+0.0+600.0+-0.0003   critic_lr = 5e-4 actor_1e-5练出来的 
reward+0.05+40.0+0.0+7.0+740.0+0.0+0.0+600.0+-0.0003   critic_lr = 5e-4 actor_5e-6练出来的 
reward+0.05+40.0+0.0+8.0+740.0+0.0+0.0+600.0+-0.0003 critic_lr = 5e-4 actor_1e-5       再尝试一次这个参数  失败
reward+0.05+40.0+0.0+9.0+740.0+0.0+0.0+600.0+-0.0003 critic_lr = 5e-4 actor_8e-6
reward+0.05+40.0+0.0+10.0+740.0+0.0+0.0+600.0+-0.0003 critic_lr = 5e-4 actor_3e-6
reward+0.05+40.0+0.0+11.0+740.0+0.0+0.0+600.0+-0.0003 critic_lr = 5e-4 actor_3e-6
reward+0.05+40.0+0.0+12.0+740.0+0.0+0.0+600.0+-0.0003 critic_lr = 5e-4 actor_5e-7 失败
reward+0.05+40.0+0.0+13.0+740.0+0.0+0.0+600.0+-0.0003 critic_lr = 5e-4 actor_2e-6 
reward+0.05+40.0+0.0+14.0+740.0+0.0+0.0+600.0+-0.0003 critic_lr = 5e-4 actor_1e-7
reward+0.05+40.0+0.0+15.0+740.0+0.0+0.0+600.0+-0.0003 critic_lr = 5e-4 actor_4e-6
reward+0.05+40.0+0.0+16.0+740.0+0.0+0.0+600.0+-0.0003 critic_lr = 5e-4 actor_4e-6
reward+0.05+40.0+0.0+17.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 5e-4 actor_lr = 1e-6训练出来的
reward+0.05+40.0+0.0+18.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 5e-4 actor_lr = 1e-6训练出来的
reward+0.05+40.0+0.0+19.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 5e-4 actor_lr = 8e-7训练出来的
reward+0.05+40.0+0.0+20.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 5e-4 actor_lr = 8e-7训练出来的
reward+0.05+40.0+0.0+21.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 5e-4 actor_lr = 9e-7训练出来的
reward+0.05+40.0+0.0+22.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 5e-4 actor_lr = 5e-7训练出来的 失败
reward+0.05+40.0+0.0+22.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 5e-4 actor_lr = 7e-7训练出来的
reward+0.05+40.0+0.0+23.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 5e-4 actor_lr = 9e-7训练出来的
reward+0.05+40.0+0.0+24.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 5e-4 actor_lr = 9e-7训练出来的
reward+0.05+40.0+0.0+25.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 5e-4 actor_lr = 9e-7训练出来的
reward+0.05+40.0+0.0+26.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 5e-4 actor_lr = 7e-7训练出来的 失败
reward+0.05+40.0+0.0+26.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 5e-4 actor_lr = 9.5e-7训练出来的
reward+0.05+40.0+0.0+27.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 5e-4 actor_lr = 8.5e-7训练出来的
reward+0.05+40.0+0.0+28.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 5e-4 actor_lr = 9e-7训练出来的
reward+0.05+40.0+0.0+51.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 3e-4 actor_lr = 3e-4训练出来的
reward+0.05+40.0+0.0+52.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 3e-4 actor_lr = 3e-4训练出来的
reward+0.05+40.0+0.0+53.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 3e-4 actor_lr = 3e-4训练出来的
reward+0.05+40.0+0.0+53.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 1e-4 actor_lr = 1e-4训练出来的


静态：
reward+0.05+40.0+0.0+54.0+740.0+0.0+0.0+600.0+-0.0003这个是critic_lr = 1e-4 actor_lr = 1e-4训练出来的
SAC之前的静态场景学习率：   critic_lr = 5e-4  # Critic的学习率
        actor_lr = 1e-5  # Actor的学习率

TD3一直是按默认的跑的
reward+0.05+40.0+0.0+0.0+740.0+0.0+0.0+600.0+-0.0003是 critic_lr = 1e-4 actor_lr=1e-4练出来的 
reward+0.05+40.0+0.0+8.0+740.0+0.0+0.0+600.0+-0.0003s是 critic_lr = 1e-5 actor_lr=1e-5练出来的  失败
reward+0.05+40.0+0.0+9.0+740.0+0.0+0.0+600.0+-0.0003 critic_lr = 5e-5 actor_5e-5 
reward+0.05+40.0+0.0+10.0+740.0+0.0+0.0+600.0+-0.0003 critic_lr = 5e-5 actor_8e-5 
reward+0.05+40.0+0.0+11.0+740.0+0.0+0.0+600.0+-0.0003 critic_lr = 8e-5 actor_8e-5
reward+0.05+40.0+0.0+12.0+740.0+0.0+0.0+600.0+-0.0003 critic_lr = 8e-5 actor_8e-5 失败
reward+0.05+40.0+0.0+13.0+740.0+0.0+0.0+600.0+-0.0003是 critic_lr = 5e-3 actor_lr=1e-3练出来的 
reward+0.05+40.0+0.0+14.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 2e-3 actor_lr=1e-3练出来的 
reward+0.05+40.0+0.0+15.0+740.0+0.0+0.0+600.0+-0.0003是 critic_lr = 1.5e-3 actor_lr=1e-3练出来的 
reward+0.05+40.0+0.0+16.0+740.0+0.0+0.0+600.0+-0.0003是 critic_lr = 1.75e-3 actor_lr=1e-3练出来的 
reward+0.05+40.0+0.0+17.0+740.0+0.0+0.0+600.0+-0.0003是 critic_lr = 1.9e-3 actor_lr=1e-3练出来的 

reward+0.05+40.0+180.0+18.0+740.0+0.0+0.0+600.0+-0.0003是 critic_lr = 2e-3 actor_lr=1e-3练出来的 
reward+0.05+40.0+180.0+14.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.2e-3 actor_lr=1e-3练出来的 
reward+0.05+40.0+180.0+19.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 2e-3 actor_lr=2e-3练出来的
reward+0.05+40.0+90.0+20.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1e-3 actor_lr=1e-3练出来的
reward+0.05+40.0+180.0+21.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1e-3 actor_lr=1e-3练出来的
reward+0.05+40.0+0.0+25.0+740.0+0.0+0.0+600.0+-0.0003是 critic_lr = 1.9e-3 actor_lr=1e-3练出来的 
reward+0.05+40.0+180.0+25.0+740.0+0.0+0.0+600.0+-0.0003是 critic_lr = 1.9e-3 actor_lr=1e-3练出来的 
reward+0.05+40.0+0.0+26.0+740.0+0.0+0.0+600.0+-0.0003是 critic_lr = 1.9e-3 actor_lr=1e-3练出来的 
reward+0.05+40.0+180.0+27.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 3e-3 actor_lr=3e-3练出来的
reward+0.05+40.0+180.0+28.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 3e-3 actor_lr=3e-3练出来的
reward+0.05+40.0+180.0+29.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 3e-3 actor_lr=3e-3练出来的
reward+0.05+40.0+180.0+30.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 2.5e-3 actor_lr=2.5e-3练出来的
reward+0.05+40.0+180.0+31.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 2.5e-3 actor_lr=2.5e-3练出来的
reward+0.05+40.0+180.0+32.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 2.5e-3 actor_lr=2.5e-3练出来的
reward+0.05+40.0+180.0+33.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 2.5e-3 actor_lr=2.5e-3练出来的

reward+0.05+40.0+180.0+34.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 2e-3 actor_lr=2e-3练出来的  失败删掉了 删掉了
reward+0.05+40.0+180.0+34.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.5e-3 actor_lr=1.5e-3练出来的
reward+0.05+40.0+180.0+35.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.8e-3 actor_lr=1.8e-3练出来的
reward+0.05+40.0+180.0+36.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.8e-3 actor_lr=1.8e-3练出来的
reward+0.05+40.0+180.0+37.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.2e-3 actor_lr=1.2e-3练出来的

reward+0.05+40.0+0.0+38.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.2e-3 actor_lr=1.2e-3练出来的
reward+0.05+40.0+0.0+39.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.2e-3 actor_lr=1.2e-3练出来的
reward+0.05+40.0+0.0+40.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.2e-3 actor_lr=1.2e-3练出来的
reward+0.05+40.0+0.0+41.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.2e-3 actor_lr=1.2e-3练出来的
reward+0.05+40.0+0.0+42.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.2e-3 actor_lr=1.2e-3练出来的
reward+0.05+40.0+0.0+43.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.2e-3 actor_lr=1.2e-3练出来的
reward+0.05+40.0+0.0+44.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.2e-3 actor_lr=1.2e-3练出来的
reward+0.05+40.0+0.0+45.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.2e-3 actor_lr=1.2e-3练出来的 从这及以下的随机种子是t*6
reward+0.05+40.0+0.0+46.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.2e-3 actor_lr=1.2e-3练出来的
reward+0.05+40.0+0.0+47.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.2e-3 actor_lr=1.2e-3练出来的
LSTM：
reward+0.05+40.0+180.0+0.0+740.0+0.0+0.0+600.0+-0.0003是 critic_lr = 1.2e-3 actor_lr=1.2e-3练出来的
reward+0.05+40.0+180.0+1.0+740.0+0.0+0.0+600.0+-0.0003是 critic_lr = 1e-3 actor_lr=1e-3练出来的


SAC:
reward+0.05+40.0+0.0+1.0+740.0+0.0+0.0+600.0+-0.0003是 critic_lr = 5e-4 actor_lr=1e-5练出来的 
reward+0.05+40.0+0.0+2.0+740.0+0.0+0.0+600.0+-0.0003是 critic_lr = 5e-5 actor_lr=1e-5练出来的 
reward+0.05+40.0+0.0+3.0+740.0+0.0+0.0+600.0+-0.0003是 critic_lr = 8e-5 actor_lr=1e-5练出来的

reward+0.05+40.0+0.0+4.0+740.0+0.0+0.0+600.0+-0.0003是 critic_lr = 5e-4 actor_lr=1e-4练出来的

reward+0.05+40.0+0.0+5.0+740.0+0.0+0.0+600.0+-0.0003+0.0001是critic_lr = 1e-3 actor_lr=1e-4练出来的
reward+0.05+40.0+0.0+5.0+740.0+0.0+0.0+600.0+-0.0003+1e-05是 critic_lr = 1e-3 actor_lr=1e-5练出来的

reward+0.05+40.0+0.0+5.0+740.0+0.0+0.0+600.0+-0.0003+1e-06是 critic_lr = 1e-3 actor_lr=1e-6练出来的
reward+0.05+40.0+0.0+6.0+740.0+0.0+0.0+600.0+-0.0003+1e-05是 critic_lr = 1e-4 actor_lr=1e-5练出来的

reward+0.05+40.0+0.0+7.0+740.0+0.0+0.0+600.0+-0.0003+1e-05是 critic_lr = 5e-3 actor_lr=1e-3练出来的

reward+0.05+40.0+0.0+8.0+740.0+0.0+0.0+600.0+-0.0003+1e-04是 critic_lr = 3e-4 actor_lr=1e-4练出来的
reward+0.05+40.0+0.0+8.0+740.0+0.0+0.0+600.0+-0.0003+1e-05是 critic_lr = 3e-4 actor_lr=1e-5练出来的
reward+0.05+40.0+0.0+8.0+740.0+0.0+0.0+600.0+-0.0003+1e-06是 critic_lr = 3e-4 actor_lr=1e-6练出来的

reward+0.05+40.0+0.0+9.0+740.0+0.0+0.0+600.0+-0.0003+1e-03是 critic_lr = 3e-3 actor_lr=1e-3练出来的
reward+0.05+40.0+0.0+9.0+740.0+0.0+0.0+600.0+-0.0003+1e-04是 critic_lr = 3e-3 actor_lr=1e-4练出来的
reward+0.05+40.0+0.0+9.0+740.0+0.0+0.0+600.0+-0.0003+1e-05是 critic_lr = 3e-3 actor_lr=1e-5练出来的
reward+0.05+40.0+0.0+9.0+740.0+0.0+0.0+600.0+-0.0003+1e-06是 critic_lr = 3e-3 actor_lr=1e-6练出来的
reward+0.05+40.0+0.0+9.0+740.0+0.0+0.0+600.0+-0.0003+1e-07是 critic_lr = 3e-3 actor_lr=1e-7练出来的
reward+0.05+40.0+0.0+9.0+740.0+0.0+0.0+600.0+-0.0003+1e-08是 critic_lr = 3e-3 actor_lr=1e-8练出来的
reward+0.05+40.0+0.0+9.0+740.0+0.0+0.0+600.0+-0.0003+1e-09是 critic_lr = 3e-3 actor_lr=1e-9练出来的
reward+0.05+40.0+0.0+9.0+740.0+0.0+0.0+600.0+-0.0003+1e-04是 critic_lr = 5e-4 actor_lr=1e-4练出来的
reward+0.05+40.0+0.0+10.0+740.0+0.0+0.0+600.0+-0.0003+1e-04是 critic_lr = 5e-4 actor_lr=1e-4练出来的
reward+0.05+40.0+0.0+11.0+740.0+0.0+0.0+600.0+-0.0003+1e-04是 critic_lr = 3e-4 actor_lr=3e-4练出来的

dynamic:
TD3.csv是45两个平均
apf是37
DDPG用33
SAC是5

static:sac:3.0

接下来是静态场景：
先是SAC：
reward+0.05+40.0+0.0+52.0+740.0+0.0+0.0+600.0+-0.0003+0.0003是两个都是3e-4练出来的
reward+0.05+40.0+0.0+53.0+740.0+0.0+0.0+600.0+-0.0003+0.0001是两个都是1e-4练出来的


reward+0.05+40.0+0.0+54.0+740.0+0.0+0.0+600.0+-0.0003+0.0001是两个都是1e-4练出来的
reward+0.05+40.0+0.0+55.0+740.0+0.0+0.0+600.0+-0.0003+0.0001是两个都是1e-4练出来的(54,55跑错了，成静态环境了)

reward+0.05+40.0+0.0+56.0+740.0+0.0+0.0+600.0+-0.0003+0.0001是两个都是1e-4练出来的
TD3:
以下随机种子是t*7
reward+0.05+40.0+180.0+51.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.5e-3 actor_lr=1.5e-3练出来的
reward+0.05+40.0+180.0+52.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.5e-3 actor_lr=1.5e-3练出来的
reward+0.05+40.0+180.0+53.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.5e-3 actor_lr=1.5e-3练出来的
reward+0.05+40.0+180.0+54.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.2e-3 actor_lr=1.2e-3练出来的
reward+0.05+40.0+180.0+54.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1e-3 actor_lr=1e-3练出来的
reward+0.05+40.0+180.0+55.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1e-3 actor_lr=1e-3练出来的
reward+0.05+40.0+180.0+56.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1e-3 actor_lr=1e-3练出来的
reward+0.05+40.0+180.0+57.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1e-3 actor_lr=1e-3练出来的
reward+0.05+40.0+180.0+58.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1e-3 actor_lr=1e-3练出来的
reward+0.05+40.0+180.0+59.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 0.8e-3 actor_lr=0.8e-3练出来的 

reward+0.05+40.0+0.0+60.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 0.9e-3 actor_lr=0.9e-3练出来的 
reward+0.05+40.0+0.0+61.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 0.8e-3 actor_lr=0.8e-3练出来的 
reward+0.05+40.0+0.0+62.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 0.8e-3 actor_lr=0.8e-3练出来的 
以下是随机种子t*10
reward+0.05+40.0+0.0+63.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 0.8e-3 actor_lr=0.8e-3练出来的 
以下是t*4:
reward+0.05+40.0+0.0+64.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 0.8e-3 actor_lr=0.8e-3练出来的 
reward+0.05+40.0+0.0+65.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 0.6e-3 actor_lr=0.6e-3练出来的 
reward+0.05+40.0+0.0+66.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 0.6e-3 actor_lr=0.6e-3练出来的 
reward+0.05+40.0+0.0+67.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 2e-3 actor_lr=2e-3练出来的 
reward+0.05+40.0+0.0+68.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.5e-3 actor_lr=1.5e-3练出来的 
reward+0.05+40.0+0.0+69.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.5e-3 actor_lr=1.5e-3练出来的 
reward+0.05+40.0+0.0+70.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.3e-3 actor_lr=1.3e-3练出来的 
reward+0.05+40.0+0.0+71.0+740.0+0.0+0.s0+600.0+-0.0003是 critic_lr = 1.8e-3 actor_lr=1.8e-3练出来的 


重新来dynamic部分：学习率都是1.2
reward+0.05+40.0+180.0+52.0+740.0+0.0+0.0+600.0+-0.0003到59都是想突破那个创新点，是一下统计critic的相似度看看能不能取消调中间掉下去那一下，发现好像不行，后来又试试用可变replay_buffer，实验了0.3，0.1，0.1，0.6的比例参数，发现都没有原本的好
reward+0.05+40.0+0.0+59.0+740.0+0.0+0.0+600.0+-0.0003  这个是实验看看用两个状态合并的训练 实验了两次都不行，现在试一试加上LSTM网络行不行   加入LSTM实验了一千多回合开始0.3以上了 前面的不加LSTM可能也行，就是太慢了没等到上升  


我突然发现之前的训练都是开了laser_state的归一化，现在先不弄来试试这个state行不行 k=6的时候发现才0.2-0.3的成功率  我发现都是d为负数，试试d的绝对值（也不行）
上面这个我怀疑是同样的位置，生成的边都不一样，况且就算边一样，每次聚类算出来的边的端点都不一样，每次都要重新训练，下次再遇到还是不会

将这个边改为保存K个边的距离以及所在的方位，是一下看到成功率在APF可以达到0.6，但是比之前的velodyne作为的状态慢很多

再将这个k个边的距离归一化试试 好像还差了一点，只到了0.4
我试试现在一个面就保存两条最近的边，然后取所有里面最近的边k个 

桌子腿会撞，因为桌子腿一个平面内只扫到一个点，聚类是三个点起步，所以桌子腿信息被抛弃掉了，现在把桌子腿信息加回来，聚类一个点起步，生成簇如果小于三个点，生成一个伪边（具体不懂），这条边放入edge参与sort边，可能会放入state里面，试一下（加进去完蛋，就剩下0.1成功率了）


总结：之前是保留一个边的三个信息，但是这个边的信息的随机性太大，state经常成没见过的状态，无法提高成功率，后来只剩下前一个信息，随机性减少很多，成功率可以上升到0.4，但是还是有随机性，第一个信息在车的同一位置扫到的不一样，所以限制了成功率


1.明天去掉圆柱体看会不会好  2.聚合成整条边


edge_feat: [-6.9598694  -9.836505   -7.68531    -6.6793942   0.85272     0.90271837
  1.2014229   1.322748  ]
edge_feat: [-6.9598713  -9.836505   -7.68531    -6.6793942   0.8527284   0.90271837
  1.2014229   1.322748  ]


  后面k位是线条到原点的方位，车的正方形是0，向右是（0，-pi/2),向左是（0，pi/2)


  现在怀疑是检测直线的精度不够问题




静态TD3：
reward+0.05+40.0+180.0+65.0+740.0+0.0+0.0+600.0+-0.0003
reward+0.05+40.0+180.0+66.0+740.0+0.0+0.0+600.0+-0.0003是四个随机圆柱体
reward+0.05+40.0+180.0+67.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体


reward+0.05+40.0+180.0+78.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1e-3 1e-3
reward+0.05+40.0+180.0+79.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1e-3 1e-3
reward+0.05+40.0+180.0+80.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1e-3 1e-3

reward+0.05+40.0+180.0+81.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.2e-3 1.2e-3
eward+0.05+40.0+180.0+82.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.2e-3 1.2e-3
eward+0.05+40.0+180.0+83.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.2e-3 1.2e-3
eward+0.05+40.0+180.0+84.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.2e-3 1.2e-3

eward+0.05+40.0+180.0+85.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.1e-3 1.1e-3

eward+0.05+40.0+180.0+87.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.2e-3 1.2e-3
eward+0.05+40.0+180.0+88.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.2e-3 1.2e-3


eward+0.05+40.0+0.0+75.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.5e-3 1.5e-3

eward+0.05+40.0+0.0+77.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.5e-4 1.5e-4
eward+0.05+40.0+0.0+78.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.5e-4 1.5e-4
eward+0.05+40.0+0.0+79.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.5e-4 1.5e-4
eward+0.05+40.0+0.0+88.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.5e-4 1.5e-4

现在想用一下新增加障碍物的静态环境：
eward+0.05+40.0+0.0+89.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.5e-4 1.5e-4
eward+0.05+40.0+0.0+90.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.5e-4 1.5e-4
eward+0.05+40.0+0.0+91.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.5e-4 1.5e-4
eward+0.05+40.0+0.0+92.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 2e-4 2e-4
eward+0.05+40.0+0.0+93.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 8e-4 8e-4
eward+0.05+40.0+0.0+94.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 0.5e-3 0.5e-3
eward+0.05+40.0+0.0+95.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 0.5e-3 0.5e-3
eward+0.05+40.0+0.0+96.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 5e-4 0.5e-4


增加到6个随机障碍物：
eward+0.05+40.0+0.0+100.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 5e-4 0.5e-4
eward+0.05+40.0+0.0+101.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 5e-4 0.5e-4
eward+0.05+40.0+0.0+102.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 5e-4 0.5e-4
eward+0.05+40.0+0.0+103.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 5e-4 0.5e-4
eward+0.05+40.0+0.0+104.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 5e-4 0.5e-4
eward+0.05+40.0+0.0+105.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 8e-4 8e-4
eward+0.05+40.0+0.0+106.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 9e-4 9e-4




dynamic:(这几个暂时和静态的记录放在了一起)
eward+0.05+40.0+0.0+77.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.2e-4 1.2e-4
eward+0.05+40.0+1.0+78.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.2e-4 1.2e-4
eward+0.05+40.0+2.0+79.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.2e-4 1.2e-4
eward+0.05+40.0+3.0+79.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1.2e-4 1.2e-4
eward+0.05+40.0+4.0+79.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 2e-4 2e-4
eward+0.05+40.0+5.0+79.0+740.0+0.0+0.0+600.0+-0.0003回到随机两个圆柱体 1e-4 1e-4




 
静态下：ddpg用54
apf-TD3用81
SAC用3
td3先借用DDPG的55


velodyne_env_line_observation是之前想用障碍物的线代替点
train_velodyne_observe也是之前想用障碍物的线代替点
velodyne_env_line_graph是之前想用障碍物的线代替点
train_velodyne_graph也是之前想用障碍物的线代替点